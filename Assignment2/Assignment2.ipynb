{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 40 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n",
        "- 20 points - Evaluate on Github Typo Corpus (for masters only)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3 or greater\n",
        "- Max is 80 points for bachelors, 100 points for masters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html).\n",
        "\n",
        "[N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "\n",
        "You may also wnat to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, Ukranian, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "Important - your project should not be a mere code copy-paste from somewhere. Implement yourself, analyze why it was suggested this way, and think of improvements/customization.\n",
        "\n",
        "Your solution should be able to perform 4 corrections per second (3-5 words in an example) on a typical cpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class Norvig_corrector: # from https://norvig.com/spell-correct.html\n",
        "\n",
        "    def words(self, text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def __init__(self):\n",
        "        self.WORDS = Counter(self.words(open('big.txt').read()))\n",
        "\n",
        "    def P(self, word): \n",
        "        N=sum(self.WORDS.values())\n",
        "        \"Probability of `word`.\"\n",
        "        return self.WORDS[word] / N\n",
        "\n",
        "    def correction(self, word): \n",
        "        \"Most probable spelling correction for word.\"\n",
        "        return max(self.candidates(word), key=self.P)\n",
        "\n",
        "    def candidates(self, word): \n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or set(word,))\n",
        "\n",
        "    def known(self, words): \n",
        "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "        return set(w for w in words if w in self.WORDS)\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word): \n",
        "        \"All edits that are two edits away from `word`.\"\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "\n",
        "class Corrector_with_weights(Norvig_corrector):\n",
        "\n",
        "    def get_bigrams(self):\n",
        "        with open(\"w2_.txt\") as f:\n",
        "            while (line := f.readline().rstrip()):\n",
        "                val, w1, w2 = line.split()\n",
        "                val = int(val)\n",
        "                # if w2 in self.stop_words:\n",
        "                #     val = 1\n",
        "                self.bigrams[w1][w2] = val\n",
        "        \n",
        "        for w1 in self.bigrams:\n",
        "            total_count = float(sum(self.bigrams[w1].values()))\n",
        "            for w2 in self.bigrams[w1]:\n",
        "                self.bigrams[w1][w2] /= total_count\n",
        "            \n",
        "    def __init__(self, w1 = 1, w2 = 0.2):\n",
        "        super().__init__()\n",
        "        self.WORDS = Counter(self.words(open('big.txt').read()))\n",
        "        self.WORDS += Counter(self.words(open('w2_.txt').read()))\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.bigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.get_bigrams()\n",
        "        self.weight1 = w1\n",
        "        self.weight2 = w2\n",
        "\n",
        "    def candidates_union(self, word): \n",
        "        # return self.candidates(word)\n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        temp = self.known([word])\n",
        "        if temp:\n",
        "            return temp, [1]\n",
        "        temp1 = self.known(self.edits1(word))\n",
        "        temp2 = self.known(self.edits2(word))\n",
        "        weights1 = [self.weight1 for i in range (len(temp1))]\n",
        "        weights2 = [self.weight2 for i in range (len(temp2))]\n",
        "        # print(weights1)\n",
        "        # print(weights2)\n",
        "        union = temp1 | temp2\n",
        "        intersection = temp1 & temp2\n",
        "        union_weights = weights1 + weights2[:len(weights2)-len(intersection)]\n",
        "        # print(union_weights)\n",
        "        \n",
        "        if union:\n",
        "            return union, union_weights\n",
        "        return {word}, [1]\n",
        "\n",
        "    def get_probable_word(self, w1_candidates, w2_candidates, w1_multipliers, w2_multipliers):\n",
        "        w1 = w1_candidates[0]\n",
        "        w2 = w2_candidates[0]\n",
        "        prob = 0\n",
        "        if (len(w1_candidates) != len(w1_multipliers)):\n",
        "            print(len(w1_candidates), len(w1_multipliers), w1_candidates, w1_multipliers)\n",
        "        if (len(w2_candidates) != len(w2_multipliers)):\n",
        "            print(len(w2_candidates), len(w2_multipliers), w2_candidates, w2_multipliers)\n",
        "        # print(len(w1_candidates), len(w1_multipliers), len(w2_candidates), len(w2_multipliers))\n",
        "        for i in range(len(w1_candidates)):\n",
        "            for j in range(len(w2_candidates)):\n",
        "                if self.bigrams[w1_candidates[i]][w2_candidates[j]] * w1_multipliers[i] * w2_multipliers[j] > prob:\n",
        "                    prob = self.bigrams[w1_candidates[i]][w2_candidates[j]] * w1_multipliers[i] * w2_multipliers[j]\n",
        "                    w1 = w1_candidates[i]\n",
        "                    w2 = w2_candidates[j]\n",
        "        return w1, w2\n",
        "    \n",
        "    def fix_string(self, string, show=False):\n",
        "        tokens = re.findall(r'\\w+',string)\n",
        "        corrections = self.fix_tokens(self.words(string))\n",
        "        if show:\n",
        "            print(tokens, corrections)\n",
        "        for token, correction in zip(tokens, corrections):\n",
        "            if token[0].isupper():\n",
        "                correction = correction[0].upper() + correction[1:]\n",
        "            idx = string.find(token)\n",
        "            string = string[:idx] + correction + string[idx + len(token):]\n",
        "        return string\n",
        "    \n",
        "    def perform_errors(self, string, prob=0.5, mistake_type_prob = 0.6):\n",
        "        tokens = re.findall(r'\\w+',string)\n",
        "        curr_idx = 0\n",
        "        for token in tokens:\n",
        "            correction = token\n",
        "            if random.random() < prob and not token.isnumeric():\n",
        "                if random.random() < mistake_type_prob:\n",
        "                    correction = random.choice(list(self.edits1(token)))\n",
        "                else: \n",
        "                    correction = random.choice(list(self.edits2(token)))\n",
        "                if correction=='':\n",
        "                    correction = token\n",
        "            if token[0].isupper():\n",
        "                correction = correction[0].upper() + correction[1:]\n",
        "            idx = string.find(token, curr_idx)\n",
        "            string = string[:idx] + correction + string[idx + len(token):]\n",
        "            curr_idx = idx + len(correction)\n",
        "        return string\n",
        "\n",
        "    def fix_tokens(self, tokens):\n",
        "        tokens = tokens[:]\n",
        "        possibilities = [[] for i in range (len(tokens))]\n",
        "        multipliers = [[] for i in range (len(tokens))]\n",
        "        for i in range (len(tokens)):\n",
        "            if self.known([tokens[i]]) or tokens[i].isnumeric():\n",
        "                possibilities[i].append(tokens[i])\n",
        "                multipliers[i].append(1)\n",
        "            else:\n",
        "                arr, mul = self.candidates_union(tokens[i])\n",
        "                # print(arr, mul)\n",
        "                possibilities[i].extend(list(arr))\n",
        "                multipliers[i].extend(mul)\n",
        "\n",
        "        ## check for 1 word in sentence\n",
        "        if len(tokens) == 1:\n",
        "            return self.correction(tokens[0])\n",
        "        \n",
        "        for i in range (len(tokens)):\n",
        "            if len(possibilities[i]) > 1:\n",
        "                if i == 0:\n",
        "                    tokens[i], _ = self.get_probable_word(possibilities[i], possibilities[i+1], multipliers[i], multipliers[i+1])\n",
        "                else:\n",
        "                    _, tokens[i] = self.get_probable_word([tokens[i-1]], possibilities[i], [1], multipliers[i])\n",
        "            else:\n",
        "                tokens[i] = possibilities[i][0]\n",
        "        return tokens\n",
        "    \n",
        "    def compare_methods(self, correct_corpus, test_corpus):\n",
        "        accuracy_Norvig = 0\n",
        "        accuracy_Corrector = 0\n",
        "        total_words = 0\n",
        "\n",
        "        for i in range (len(test_corpus)):\n",
        "\n",
        "            correct_tokens = self.words(correct_corpus[i])\n",
        "            total_words += len(correct_tokens)\n",
        "            test_tokens = self.words(test_corpus[i])\n",
        "            corrector_answer = self.fix_tokens(test_tokens)\n",
        "\n",
        "            for j in range (len(correct_tokens)):\n",
        "                if self.correction(test_tokens[j]) == correct_tokens[j]:\n",
        "                    accuracy_Norvig+=1\n",
        "\n",
        "                if corrector_answer[j] == correct_tokens[j]:\n",
        "                    accuracy_Corrector+=1\n",
        "                    \n",
        "        accuracy_Corrector /= total_words\n",
        "        accuracy_Norvig /= total_words\n",
        "        print(f\"Norvig's corrector accuracy:\\t{accuracy_Norvig}\\nContext corrector accuracy:\\t{accuracy_Corrector}\")\n",
        "        \n",
        "    def context_fix(self, corpus):\n",
        "        fixed_corpus = []\n",
        "        for string in corpus:\n",
        "            fixed_corpus.append(self.fix_string(string))\n",
        "        return fixed_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Corrector(Norvig_corrector):\n",
        "\n",
        "    def get_bigrams(self):\n",
        "        with open(\"w2_.txt\") as f:\n",
        "            while (line := f.readline().rstrip()):\n",
        "                val, w1, w2 = line.split()\n",
        "                val = int(val)\n",
        "                # if w2 in self.stop_words:\n",
        "                #     val = 1\n",
        "                self.bigrams[w1][w2] = val\n",
        "        \n",
        "        for w1 in self.bigrams:\n",
        "            total_count = float(sum(self.bigrams[w1].values()))\n",
        "            for w2 in self.bigrams[w1]:\n",
        "                self.bigrams[w1][w2] /= total_count\n",
        "            \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.WORDS = Counter(self.words(open('big.txt').read()))\n",
        "        self.WORDS += Counter(self.words(open('w2_.txt').read()))\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.bigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.get_bigrams()\n",
        "\n",
        "    def candidates_union(self, word): \n",
        "        return self.candidates(word)\n",
        "\n",
        "    def get_probable_word(self, w1_candidates, w2_candidates):\n",
        "        w1 = w1_candidates[0]\n",
        "        w2 = w2_candidates[0]\n",
        "        prob = 0\n",
        "        # print(len(w1_candidates), len(w1_multipliers), len(w2_candidates), len(w2_multipliers),)\n",
        "        for i in range(len(w1_candidates)):\n",
        "            for j in range(len(w2_candidates)):\n",
        "                if self.bigrams[w1_candidates[i]][w2_candidates[j]] > prob:\n",
        "                    prob = self.bigrams[w1_candidates[i]][w2_candidates[j]]\n",
        "                    w1 = w1_candidates[i]\n",
        "                    w2 = w2_candidates[j]\n",
        "        return w1, w2\n",
        "    \n",
        "    def fix_string(self, string, show=False):\n",
        "        tokens = re.findall(r'\\w+',string)\n",
        "        corrections = self.fix_tokens(self.words(string))\n",
        "        if show:\n",
        "            print(tokens, corrections)\n",
        "        for token, correction in zip(tokens, corrections):\n",
        "            if token[0].isupper():\n",
        "                correction = correction[0].upper() + correction[1:]\n",
        "            idx = string.find(token)\n",
        "            string = string[:idx] + correction + string[idx + len(token):]\n",
        "        return string\n",
        "    \n",
        "    def perform_errors(self, string, prob=0.5, mistake_type_prob = 0.6):\n",
        "        tokens = re.findall(r'\\w+',string)\n",
        "        curr_idx = 0\n",
        "        for token in tokens:\n",
        "            correction = token\n",
        "            if random.random() < prob and not token.isnumeric():\n",
        "                if random.random() < mistake_type_prob:\n",
        "                    correction = random.choice(list(self.edits1(token)))\n",
        "                else: \n",
        "                    correction = random.choice(list(self.edits2(token)))\n",
        "                if correction=='':\n",
        "                    correction = token\n",
        "            if token[0].isupper():\n",
        "                correction = correction[0].upper() + correction[1:]\n",
        "            idx = string.find(token, curr_idx)\n",
        "            string = string[:idx] + correction + string[idx + len(token):]\n",
        "            curr_idx = idx + len(correction)\n",
        "        return string\n",
        "\n",
        "    def fix_tokens(self, tokens):\n",
        "        tokens = tokens[:]\n",
        "        possibilities = [[] for i in range (len(tokens))]\n",
        "        for i in range (len(tokens)):\n",
        "            if self.known([tokens[i]]) or tokens[i].isnumeric():\n",
        "                possibilities[i].append(tokens[i])\n",
        "            else:\n",
        "                arr = self.candidates_union(tokens[i])\n",
        "                possibilities[i].extend(list(arr))\n",
        "\n",
        "        ## check for 1 word in sentence\n",
        "        if len(tokens) == 1:\n",
        "            return self.correction(tokens[0])\n",
        "        \n",
        "        for i in range (len(tokens)):\n",
        "            if len(possibilities[i]) > 1:\n",
        "                if i == 0:\n",
        "                    tokens[i], _ = self.get_probable_word(possibilities[i], possibilities[i+1])\n",
        "                else:\n",
        "                    _, tokens[i] = self.get_probable_word([tokens[i-1]], possibilities[i])\n",
        "            else:\n",
        "                tokens[i] = possibilities[i][0]\n",
        "        return tokens\n",
        "    \n",
        "    def compare_methods(self, correct_corpus, test_corpus):\n",
        "        accuracy_Norvig = 0\n",
        "        accuracy_Corrector = 0\n",
        "        total_words = 0\n",
        "\n",
        "        for i in range (len(test_corpus)):\n",
        "\n",
        "            correct_tokens = self.words(correct_corpus[i])\n",
        "            total_words += len(correct_tokens)\n",
        "            test_tokens = self.words(test_corpus[i])\n",
        "            corrector_answer = self.fix_tokens(test_tokens)\n",
        "            # print(len(correct_tokens), len(test_tokens), len(corrector_answer))\n",
        "            try:\n",
        "                for j in range (len(correct_tokens)):\n",
        "                    if self.correction(test_tokens[j]) == correct_tokens[j]:\n",
        "                        accuracy_Norvig+=1\n",
        "\n",
        "                    if corrector_answer[j] == correct_tokens[j]:\n",
        "                        accuracy_Corrector+=1\n",
        "            except:\n",
        "                print(\"Some error\")\n",
        "                print(correct_tokens, test_tokens, corrector_answer)\n",
        "                ans = []\n",
        "                for j in range (len(correct_tokens)):\n",
        "                    ans.append(self.correction(test_tokens[j]))\n",
        "                print(ans)\n",
        "                print(len(correct_tokens), len(test_tokens), len(corrector_answer), len(ans))\n",
        "        accuracy_Corrector /= total_words\n",
        "        accuracy_Norvig /= total_words\n",
        "        print(f\"Norvig's corrector accuracy:\\t{accuracy_Norvig}\\nContext corrector accuracy:\\t{accuracy_Corrector}\")\n",
        "        \n",
        "    def context_fix(self, corpus):\n",
        "        fixed_corpus = []\n",
        "        for string in corpus:\n",
        "            fixed_corpus.append(self.fix_string(string))\n",
        "        return fixed_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "In your implementation you will need to decide which ngram dataset to use, which weights to assign for edit1, edit2 or absent words probabilities, beam search parameters and etc. Write down justificaitons for these choices."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "I decided to use bigram dataset because it is not as sparse as other ngrams datasets and it also cover sentences that consists of two words (f.e. \"Go outside!\" or \"Shut up.\"). \n",
        "I tried two different approaches:\n",
        "- (Corrector with weights) Assign some additional weights for edit 2 and therefore change candidate function in order to operate with union of sets from edit1 and edit2. However performance for that approach is not better than another one.\n",
        "- (Corrector) Do not assign any weights to edit1 and edit2 and operate with candidate function from Norvig's solution.\n",
        "In any way both solutions wasn't able to perform better than Norvig's.  \n",
        "Speaking about probabilities and for absent words. In all previously mentioned solutions if we face unknown word it firstly would consider it as a mistake and try to edit it. If there is no possibilities to convert this unknown word to known one through edit1 or edit2, then we leave this word as it is. In that case we have only one variant -> bigrams probabilities doesn't really matter (the maximum probability is 0 for any pair of an unknown word and any other word).\n",
        "\n",
        "Now let's talk about beam search, to be honest I haven't came to any idea about how to use it in that problem.\n",
        "\n",
        "Finally, let's consider performance of Corrector on texts with different error probabilities. The performance is better when the errors in the test data are mostly of the type edit1, this is because the word with the error edit2 has a larger number of possible candidates than with edit1. If there are few errors, the performance is not bad, but with an increase in the error rate, the probability of getting into a chain of incorrectly selected words increases. In this scenario, a larger number of candidates for some words means a greater probability of a significant change in this sentence. This could probably be fixed a bit by using several ngrams to determine the best candidate, but this leads us to exhausting tuning of hyperparameters (weights for each ngram) and other problems (for example, that with more n in ngrams, the matrix will be more sparse, therefore, this will require advanced smoothing, otherwise it will not lead to significant changes in the results).\n",
        "\n",
        "One more note: If Corrector or Corrector with weights comes across a sentence consisting of a single word, it uses Norvig's solution to correct it if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "corrector = Corrector()\n",
        "corrector_with_weights = Corrector_with_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "s = \"If you were poking around RT a week and a half or so ago, you might have come across a little poll we were taking on the site to try and determine the Scariest Movie Ever. Based on other lists and suggestions from the RT staff, we pulled together 40 of the scariest movies ever made and asked you to vote for the one that terrified you the most. As it happens, a British broadband service comparison website decided to conduct a science experiment to determine the same thing, and their results were… surprising, to say the least. Did Rotten Tomatoes readers agree with the findings? Read on to find out what our fans determined were the 10 Scariest Horror Movies Ever.\"\n",
        "s += \"You mr-director Aray not agree that The Exorcist is the scariest movie ever, but it probably also isn’t much of a surprise to see it at the top of our list — with a whopping 19% of all the votes cast. William Friedkin’s adaptation of the eponymous novel about a demon-possessed child and the attempts to banish said demon became the highest-grossing R-rated horror film ever and the first to be nominated for Best Picture at the Oscars (it earned nine other nominations and took home two trophies). But outside of its critical and commercial bona fides, the film is well-known for the mass hysteria it inspired across the country, from protests over its controversial subject matter to widespread reports of nausea and fainting in the audience. Its dramatic pacing and somewhat dated effects may seem quaint compared to some contemporary horror, but there’s no denying the power the film continues to have over those who see it for the first time.\"\n",
        "s += \"Writei Aster made a huge splash with his feature directorial debut, a dark family drama about the nature of grief couched within a supernatural horror film. Toni Collette earned a spot in the pantheon of great Oscar snubs with her slowly-ratcheted-up-to-11 performance as bedeviled mother Annie, but the movie’s biggest shock came courtesy of… Well, we won’t spoil that here. Suffice it to say Hereditary struck such a nerve with moviegoers that it instantly turned Aster into a director to watch and shot up to second place on our list.\"\n",
        "s += 'Somewhat mystifyingly, some top-secret algorithmic function in DreamWorks Animation’s audience-reaction data analysis software has decreed that yet another comeback is in order for the sort of OK-ish and meh-plus character of Puss in Boots, smokily voiced by Antonio Banderas, originally seen in 2004 in Shrek 2, and then in the 2010 spinoff feature Puss in Boots. The numbers have come chuntering out of the side of some giant IBM-style computer, the suits have frowningly inspected them, and another tranche of Puss in Boots content has been greenlit. Once again, debonair outlaw Puss in Boots – a sort of cleaned-up southern European version of Jack Sparrow – is having sword-twirling adventures, again in the company of his paramour Kitty Softpaws (Salma Hayek); but now PiB must confront his own mortality, having used up eight of his nine lives. He is on a quest to put off the evil hour by finding the legendary wishing star which once fell to earth like a comet; he and Kitty join forces with the perky mutt Perrito (Harvey Guillén), but must battle other fairytale/nursery-rhyme honchos, including a Cockney crime family in the form of Goldilocks (Florence Pugh) and the Three Bears (Olivia Colman, Ray Winstone and Samson Kayo), and “Big” Jack Horner (John Mulaney) – to whom all the funny lines are given. Wagner Moura voices the Wolf, who is the grim reaper, wielding a couple of sickles.'\n",
        "s += 'Really, this movie is a huge 102-minute additional scene, something that would go on the extras package of a Blu-ray edition of the previous Puss in Boots film, or possibly get its own video-on-demand release. It feels like something to put on your TV or iPad to pacify a toddler; nothing wrong with that, of course, and many stressed parents would call it the noblest artistic calling. But how bland and forgettable this film is, without in the smallest way harnessing the real performing power of Banderas, Colman, Pugh, Winstone et al.'\n",
        "corpus = sent_tokenize(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For probability of mistake = 0.1 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.9114799446749654\n",
            "Context corrector accuracy:\t0.9087136929460581\n",
            "\n",
            "For probability of mistake = 0.1 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.9308437067773168\n",
            "Context corrector accuracy:\t0.9266943291839558\n",
            "\n",
            "For probability of mistake = 0.1 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.9363762102351314\n",
            "Context corrector accuracy:\t0.9308437067773168\n",
            "\n",
            "For probability of mistake = 0.1 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.9336099585062241\n",
            "Context corrector accuracy:\t0.9294605809128631\n",
            "\n",
            "For probability of mistake = 0.2 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.8907330567081605\n",
            "Context corrector accuracy:\t0.8824343015214384\n",
            "\n",
            "For probability of mistake = 0.2 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.8824343015214384\n",
            "Context corrector accuracy:\t0.8755186721991701\n",
            "\n",
            "For probability of mistake = 0.2 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.8921161825726142\n",
            "Context corrector accuracy:\t0.8865836791147994\n",
            "\n",
            "For probability of mistake = 0.2 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.9156293222683264\n",
            "Context corrector accuracy:\t0.9142461964038727\n",
            "\n",
            "For probability of mistake = 0.30000000000000004 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.8367911479944675\n",
            "Context corrector accuracy:\t0.8160442600276625\n",
            "\n",
            "For probability of mistake = 0.30000000000000004 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.8616874135546335\n",
            "Context corrector accuracy:\t0.8437067773167358\n",
            "\n",
            "For probability of mistake = 0.30000000000000004 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.8810511756569848\n",
            "Context corrector accuracy:\t0.8699861687413555\n",
            "\n",
            "For probability of mistake = 0.30000000000000004 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.8921161825726142\n",
            "Context corrector accuracy:\t0.8810511756569848\n",
            "\n",
            "For probability of mistake = 0.4 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.7869986168741355\n",
            "Context corrector accuracy:\t0.7621023513139695\n",
            "\n",
            "For probability of mistake = 0.4 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.8201936376210235\n",
            "Context corrector accuracy:\t0.7869986168741355\n",
            "\n",
            "For probability of mistake = 0.4 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.8312586445366529\n",
            "Context corrector accuracy:\t0.7925311203319502\n",
            "\n",
            "For probability of mistake = 0.4 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.8589211618257261\n",
            "Context corrector accuracy:\t0.8492392807745505\n",
            "\n",
            "For probability of mistake = 0.5 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.7510373443983402\n",
            "Context corrector accuracy:\t0.7289073305670816\n",
            "\n",
            "For probability of mistake = 0.5 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.7690179806362379\n",
            "Context corrector accuracy:\t0.7413554633471646\n",
            "\n",
            "For probability of mistake = 0.5 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.8035961272475796\n",
            "Context corrector accuracy:\t0.7704011065006916\n",
            "\n",
            "For probability of mistake = 0.5 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.8423236514522822\n",
            "Context corrector accuracy:\t0.8326417704011065\n",
            "\n",
            "For probability of mistake = 0.6000000000000001 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.7026279391424619\n",
            "Context corrector accuracy:\t0.6473029045643154\n",
            "\n",
            "For probability of mistake = 0.6000000000000001 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.7289073305670816\n",
            "Context corrector accuracy:\t0.6763485477178424\n",
            "\n",
            "For probability of mistake = 0.6000000000000001 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.7634854771784232\n",
            "Context corrector accuracy:\t0.7247579529737206\n",
            "\n",
            "For probability of mistake = 0.6000000000000001 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.7939142461964038\n",
            "Context corrector accuracy:\t0.7648686030428768\n",
            "\n",
            "For probability of mistake = 0.7000000000000001 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.6583679114799447\n",
            "Context corrector accuracy:\t0.5878284923928078\n",
            "\n",
            "For probability of mistake = 0.7000000000000001 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.69432918395574\n",
            "Context corrector accuracy:\t0.6196403872752421\n",
            "\n",
            "For probability of mistake = 0.7000000000000001 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.7247579529737206\n",
            "Context corrector accuracy:\t0.6915629322268326\n",
            "\n",
            "For probability of mistake = 0.7000000000000001 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.7994467496542186\n",
            "Context corrector accuracy:\t0.7621023513139695\n",
            "\n",
            "For probability of mistake = 0.8 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.6265560165975104\n",
            "Context corrector accuracy:\t0.5408022130013831\n",
            "\n",
            "For probability of mistake = 0.8 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.6625172890733056\n",
            "Context corrector accuracy:\t0.5629322268326418\n",
            "\n",
            "For probability of mistake = 0.8 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.7206085753803596\n",
            "Context corrector accuracy:\t0.6376210235131397\n",
            "\n",
            "For probability of mistake = 0.8 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.7648686030428768\n",
            "Context corrector accuracy:\t0.7081604426002767\n",
            "\n",
            "For probability of mistake = 0.9 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.5905947441217151\n",
            "Context corrector accuracy:\t0.45228215767634855\n",
            "\n",
            "For probability of mistake = 0.9 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.6583679114799447\n",
            "Context corrector accuracy:\t0.5601659751037344\n",
            "\n",
            "For probability of mistake = 0.9 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.6970954356846473\n",
            "Context corrector accuracy:\t0.607192254495159\n",
            "\n",
            "For probability of mistake = 0.9 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.7441217150760719\n",
            "Context corrector accuracy:\t0.669432918395574\n",
            "\n",
            "For probability of mistake = 1.0 and probability of performing edit1 type of mistake = 0.1\n",
            "Norvig's corrector accuracy:\t0.5352697095435685\n",
            "Context corrector accuracy:\t0.38865836791147995\n",
            "\n",
            "For probability of mistake = 1.0 and probability of performing edit1 type of mistake = 0.33\n",
            "Norvig's corrector accuracy:\t0.5698478561549101\n",
            "Context corrector accuracy:\t0.44813278008298757\n",
            "\n",
            "For probability of mistake = 1.0 and probability of performing edit1 type of mistake = 0.66\n",
            "Norvig's corrector accuracy:\t0.6556016597510373\n",
            "Context corrector accuracy:\t0.5698478561549101\n",
            "\n",
            "For probability of mistake = 1.0 and probability of performing edit1 type of mistake = 1\n",
            "Norvig's corrector accuracy:\t0.7385892116182573\n",
            "Context corrector accuracy:\t0.640387275242047\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mistake_probability = [0.1*i for i in range(1, 11)]\n",
        "edit1_prob = [0.1, 0.33, 0.66, 1]\n",
        "for i in range (10):\n",
        "    for j in range(4):\n",
        "        corpus_with_errors=[]\n",
        "        for sentence in corpus:\n",
        "            corpus_with_errors.append(corrector.perform_errors(sentence, prob=mistake_probability[i], mistake_type_prob=edit1_prob[j]))\n",
        "        print(f\"For probability of mistake = {mistake_probability[i]} and probability of performing edit1 type of mistake = {edit1_prob[j]}\")\n",
        "        corrector.compare_methods(corpus, corpus_with_errors)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21 21\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ac xae vh ajaaaa k vga aaaj ap lao yof c apn aauaa n ta ozfn yj ar aq ta qd'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "some_s = 'a a a aaaaaa a a aaa a a of a a aaaa a a of aa a a a a'\n",
        "some_err = corrector.perform_errors(some_s, 1)\n",
        "print(len(some_s.split()), len(some_err.split()))\n",
        "some_err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['If you were poking around Ra i week alnd t half or so gago, yod mighth have cimec across a lirttle pol we were taking on the site to try and determine the Scariest PMovih Ever.',\n",
              " 'Jasnd on other lists and suggestiond from he RT staff, we pulled together 40 of the scaryesq movies ever made and asked ydu eto vote for the one that terrified wxou bhe most.',\n",
              " 'OhAs it happens, a British broadband service comparison website decidedu to conduct a science expeaiyent to determioe uhe same thing, and their results were… esurprisinga, to aay the least.',\n",
              " 'Did Rotten Tomatoes zrvaders vgree with the findings?',\n",
              " 'Rfead on uio fijnd out what oub farns deteyimined were thge 10 Scariest Horror Movies Ever.You mr-dirbctor Avrawy not agree thyast The Exorcisa is the jscoariest movie ever, nut it probably also isn’t mgsuch ofy a surprise to wvee it at tjhe top obf our list — with a whopiing 19% of all thwe vontjes cast.',\n",
              " 'William Friedkin’dss adxptation ofdr the eponymous nnvel about a demon-possessed child and the attempts to banish said dexdmon became the highest-grossing R-srated horhrov film ever and thz first to be nominated for Best Pigcturej at the Oscars (k earned nine otheur nominations pand tooa home fwp trophies).',\n",
              " 'But outside sotf its critical aynd commercial boaq fides, the film is well-known for the mbss hysteria it inspired across phe countroy, from protests over its controversial subject mattesr ta widespread reports of nausea and fainting in twhe audience.',\n",
              " 'UIjs dramjatpic pacing and somewhat dated effects mav seem aquaint compared to some cyntemporary horrsor, but there’q no denying the power thxe film confinues wrto have over uhose gwho see it fgr the first timde.Writei Axhter umade a hrugg sglaxh with his featurue directorial debut, ez dark famia drama about the nature of grifef couchmef witvn a supernatural horror film.',\n",
              " 'Toni Collette earned al spot in the pantheon of great Oscar snubs wit her slowly-ratcheted-up-to-11 perfqrmance as bedevgiled mother Annie, but thn nmovie’sw biggest shock came courtesey wlf… Well, qwe won’t spoil that here.',\n",
              " 'Suffice kt to sabv Hereditary struck suczh a nerve with moviegoers ehat fbt instantly turned Aster intgo fa director to watch and shot uzt tfo second pplace an ourp list.Somewhat mdystifyingly, some ytowp-secret algorithmic function xnw DreamWorks Animation’s audience-reaction data arnalysis software has decreed teatx qet another comeback isk in oyrder for the sortl om OKa-ish and meh-plus charwcter ofr Puss in Bowhts, smokily voicqed by Antfnio Bandejas, orignnally seen in 2004 in Shrek 2, and then vjin the 2010 spinoffc cfeature Puss wuin Bootsm.',\n",
              " 'The numbers have comej chuntering out xf the side of some gihnjt IBM-styxze computer, the suits have frownlngjly inivected them, and anotsher atranche of Uusws in Booas content has been gueenlit.',\n",
              " 'Once again, debonair outlaw Puss siin Booytsj – z sort of cleaned-up southern Euuropeay versiok of Jack Spatrow – isy having sword-twirling adventures, again in the comtpnny zof hibs pharamour Kitpty Softpaws (Salma Hayek); but now IPiB must confront his own mortality, having used up eight of hdis nine lives.',\n",
              " 'He iozs on a lquest to put off htek evil houtr by finding the legendary wioshing star which oncoe fell toi earth like qr coget; he and Kityy join forces with the perky mutt Perrito (Harvey Guillén), vut must battfle other fairytalj/nurksefy-rhyme sonchos, including a Cockney crmmev familry in tde form of Goldixlocks (Florence Pugh) and tsehe Three Bears (Olivia Colman, Rap Wicnstone anmd Samson Kayo), and “Oig” Jack Horner (John Mulaney) – to whom all the funtny lines are given.',\n",
              " 'Wagnir Moura voices the CWzlf, who fs the egriw reaper, wielding a couple owf sickles.Realxy, thlds movie is a hube 102-minute additional scene, somntfhing that xwould go on the extras pacrage of naq Blu-raay edivion ozf taheb rpevious Puss in Boots fiwlm, or possibly get its cwn ovidho-on-demand release.',\n",
              " 'It feels lsike something yv put an your TV hor iPad to pacify b toddler; nothing wrong with that, bf course, tand many stressed pirents woold calzl xait the noblest artistic calling.',\n",
              " 'But how bland pani forgettable this filemu ins, without tin tkce sumabllest way harckessing the real permforming jower mof Bsandetas, Colman, Pugh, Winstone etd al.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_with_errors = []\n",
        "for sentence in corpus:\n",
        "    corpus_with_errors.append(corrector.perform_errors(sentence, prob=0.33))\n",
        "corpus_with_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['If you were poking around Ra i week and t half or so ago, you might have come across a little pol we were taking on the site to try and determine the Scariest Movie Ever.',\n",
              " 'Based on other lists and suggestions from he Rt staff, we pulled together 40 of the scariest movies ever made and asked you to vote for the one that terrified whod she most.',\n",
              " 'Has it happens, a British broadband service comparison website decided to conduct a science experiment to determine the same thing, and their results were… surprising, to say the least.',\n",
              " 'Did Rotten Tomatoes traders agree with the findings?',\n",
              " 'Read on io fiend out what our fans determined were the 10 Scariest Horror Movies Ever.You mr-director Array not agree that The Exorcist is the scariest movie ever, nut it probably also isn’t much of a surprise to weve it at the top of our list — with a whopping 19% of all the votes cast.',\n",
              " 'William Friedkin’dss adaptation fdr the eponymous navel about a demon-possessed child and the attempts to banish said demon became the highest-grossing R-stated horrow film ever and the first to be nominated for Best Picture at the Oscars (k earned nine other nominations pand toot home ftp trophies).',\n",
              " 'But outside soto its critical and commercial boar fides, the film is well-known for the mass hysteria it inspired across the country, from protests over its controversial subject matter ta widespread reports of nausea and fainting in the audience.',\n",
              " 'Uis dramatic pacing and somewhat dated effects may seem quaint compared to some contemporary horror, but there’q no denying the power the film continues wto have over those who see it for the first time.Writes Auther made a huge slash with his feature directorial debut, ev dark tamil drama about the nature of grief couched whiten a supernatural horror film.',\n",
              " 'Toni Collette earned al spot in the pantheon of great Oscar subs wit her slowly-ratcheted-up-to-11 performance as bedeviled mother Annie, but the movie’sw biggest shock came courtesy wlc… Well, we won’t spoil that here.',\n",
              " 'Suffice it to sabo Hereditary struck such a nerve with moviegoers eat fat instantly turned Astor intro fa director to watch and shot azt sfo second place an burp list.Somewhat l, some top-secret algorithms function nw Dreamworks Animation’s audience-reaction data analysis software has decreed that set another comeback is in order for the sort om Oka-is and me-plus character ofr Puss in Boots, smokey voiced by Antonio Banderas, originally seen in 2004 in Shrek 2, and then jin the 2010 spinoff feature Puss ruin Boots.',\n",
              " 'The numbers have come countering out of the side of some giant Ibm-style computer, the suits have frowningly inverted them, and another attache of Buses in Books content has been i.',\n",
              " 'Once again, debonair outlaw Puss sein Boots – z sort of cleaned-up southern European version of Jack Sparrow – isn having sword-twirling adventures, again in the company of his a Kitty P (Salma Hayek); but now Ipn must confront his own mortality, having used up eight of his nine lives.',\n",
              " 'He ions on a quest to put off tek evil hour by finding the legendary wishing star which once fell toi earth like mr cogent; he and Kitty join forces with the perky mutt Cerrito (Harvey Guillen), vat must battle other fairytale/nursery-rhyme ponchos, including a Cockney crimes family in the form of I (Florence Pugo) and the Three Bears (Olivia Corman, Rap Winston and Samson Mayo), and “Big” Jack Horner (John Dulaney) – to whom all the funny lines are given.',\n",
              " 'Wagner Moura voices the Wolf, who fs the egria reaper, wielding a couple of pickles.Realty, thuds movie is a huge 102-minute additional scene, something that would go on the extras package of nat Blu-raab edition of the previous Puss in Boots film, or possibly get its own video-on-demand release.',\n",
              " 'It feels like something ev put an your Tv or ipad to pacify b toddler; nothing wrong with that, if course, and many stressed parents would call sait the noblest artistic calling.',\n",
              " 'But how bland pali forgettable this film ins, without tin tice smallest way harnessing the real performing lower of Banderas, Corman, Pugo, Winston epd al.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.context_fix(corpus_with_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.8450899031811895\n"
          ]
        }
      ],
      "source": [
        "corrector.compare_methods(corpus, corpus_with_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for weight for edit2 0.1\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7952973720608575\n",
            "\n",
            "for weight for edit2 0.2\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7980636237897649\n",
            "\n",
            "for weight for edit2 0.30000000000000004\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7980636237897649\n",
            "\n",
            "for weight for edit2 0.4\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 0.5\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 0.6000000000000001\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 0.7000000000000001\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 0.8\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 0.9\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n",
            "for weight for edit2 1.0\n",
            "Norvig's corrector accuracy:\t0.8575380359612724\n",
            "Context corrector accuracy:\t0.7994467496542186\n",
            "\n"
          ]
        }
      ],
      "source": [
        "l = [0.1*i for i in range(1, 11)]\n",
        "for i in range (10):\n",
        "    print(f\"for weight for edit2 {l[i]}\")\n",
        "    corrector_with_weights.weight2 = l[i]\n",
        "    corrector_with_weights.compare_methods(corpus, corpus_with_errors)\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ara', 'ara', 'aunmt', 'likes', 'littl', 'bous'] ['ara', 'ara', 'aunt', 'likes', 'little', 'boys']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Ara-ara aunt likes little boys.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.fix_string(\"Ara-ara aunmt likes littl bous.\", show=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'ws', 'bor', 'on', 'Novber', '25th'] ['i', 'was', 'born', 'on', 'november', '25th']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I was born on November 25th'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.fix_string(\"I ws bor on Novber 25th\", show=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Shhut', 'uip'] ['shut', 'up']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Shut up!'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.fix_string(\"Shhut uip!\", show=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ded9ff831dc04cdbad0ec525ef3c5f9b778b42d4743377cc7edc0c243e84425"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
